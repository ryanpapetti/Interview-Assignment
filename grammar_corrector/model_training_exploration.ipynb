{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanpapetti/Desktop/Programming/Professional/Feedback Fruits/Interview Assignment/venv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "06/12/2022 01:33:22 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from happytransformer import HappyTextToText\n",
    "\n",
    "import errant,csv\n",
    "ANNOTATOR = errant.load('en')\n",
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846e5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     850 MB |     850 MB |     850 MB |       0 B  |\n",
      "|       from large pool |     850 MB |     850 MB |     850 MB |       0 B  |\n",
      "|       from small pool |       0 MB |       0 MB |       0 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     850 MB |     850 MB |     850 MB |       0 B  |\n",
      "|       from large pool |     850 MB |     850 MB |     850 MB |       0 B  |\n",
      "|       from small pool |       0 MB |       0 MB |       0 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     938 MB |     938 MB |     938 MB |       0 B  |\n",
      "|       from large pool |     936 MB |     936 MB |     936 MB |       0 B  |\n",
      "|       from small pool |       2 MB |       2 MB |       2 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   89795 KB |  103887 KB |  601470 KB |  511675 KB |\n",
      "|       from large pool |   87936 KB |  102016 KB |  599424 KB |  511488 KB |\n",
      "|       from small pool |    1859 KB |    2046 KB |    2046 KB |     187 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     257    |     257    |     257    |       0    |\n",
      "|       from large pool |     193    |     193    |     193    |       0    |\n",
      "|       from small pool |      64    |      64    |      64    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     257    |     257    |     257    |       0    |\n",
      "|       from large pool |     193    |     193    |     193    |       0    |\n",
      "|       from small pool |      64    |      64    |      64    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      44    |      44    |      44    |       0    |\n",
      "|       from large pool |      43    |      43    |      43    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      44    |      44    |      44    |       0    |\n",
      "|       from large pool |      43    |      43    |      43    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# del variables\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e34a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "  (\" .\", \".\"), \n",
    "  (\" ,\", \",\"),\n",
    "  (\" '\", \"'\"),\n",
    "  (\" ?\", \"?\"),\n",
    "  (\" !\", \"!\"),\n",
    "  (\" :\", \"!\"),\n",
    "  (\" ;\", \"!\"),\n",
    "  (\" n't\", \"n't\"),\n",
    "  (\" v\", \"n't\"),\n",
    "  (\" 've\", \"'ve\"),\n",
    "  (\"2 0 0 6\", \"2006\"),\n",
    "  (\"5 5\", \"55\"),\n",
    "  (\"4 0 0\", \"400\"),\n",
    "  (\"1 7-5 0\", \"1750\"),\n",
    "  (\"2 0 %\", \"20%\"),\n",
    "  (\"5 0\", \"50\"),\n",
    "  (\"1 2\", \"12\"),\n",
    "  (\"1 0\", \"10\"),\n",
    "  ('\" ballast water', '\"ballast water') #specific example\n",
    "]\n",
    "\n",
    "def remove_excess_spaces(text):\n",
    "  for rep in replacements:\n",
    "    text = text.replace(rep[0], rep[1])\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751fb1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(csv_path, dataset):\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writter = csv.writer(csvfile)\n",
    "        writter.writerow([\"input\", \"target\"])\n",
    "        for case in dataset:\n",
    "            input_text = \"grammar: \" + case[\"sentence\"]\n",
    "            for correction in case[\"corrections\"]:\n",
    "                # a few of the cases are blank strings. So we'll skip them\n",
    "                if input_text and correction:\n",
    "                    writter.writerow([remove_excess_spaces(input_text), remove_excess_spaces(correction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92576dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
    "eval_dataset = load_dataset(\"jfleg\", split='test[:]')\n",
    "generate_csv(\"data/jfleg_train.csv\", train_dataset)\n",
    "generate_csv(\"data/jfleg_eval.csv\", eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231760d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/12/2022 01:37:08 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/ryanpapetti/.cache/huggingface/datasets/csv/default-6bfa9a9f53efaa7a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5577e49bfc54ff992673b226ebcd2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08184d368efb43518c2a768eacfe9f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ryanpapetti/.cache/huggingface/datasets/csv/default-6bfa9a9f53efaa7a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c17525b966a49088cb80ae0b1e99b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaff46c289344e7a53d1d34b9a8821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2988\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df812847f04545edbfb0ecee33625aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loss:  1.4225281476974487\n"
     ]
    }
   ],
   "source": [
    "before_result = happy_tt.eval(\"data/jfleg_eval.csv\")\n",
    "print(\"Before loss: \", before_result.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd602824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/12/2022 02:02:32 - INFO - happytransformer.happy_transformer -   Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87d5faf952b49c3bb5acaa728cf3e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f123301cb051458f83cda409a0d18560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/12/2022 02:02:33 - INFO - happytransformer.happy_transformer -   Training...\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 3016\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9b2ff62935442cad17eea764424313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4025, 'learning_rate': 4.792771883289125e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4068, 'learning_rate': 4.5855437665782495e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3975, 'learning_rate': 4.3783156498673745e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4303, 'learning_rate': 4.171087533156499e-05, 'epoch': 0.66}\n",
      "{'loss': 0.4079, 'learning_rate': 3.963859416445623e-05, 'epoch': 0.83}\n",
      "{'loss': 0.4135, 'learning_rate': 3.756631299734748e-05, 'epoch': 0.99}\n",
      "{'loss': 0.2932, 'learning_rate': 3.549403183023873e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3331, 'learning_rate': 3.3421750663129974e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3546, 'learning_rate': 3.1349469496021224e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3377, 'learning_rate': 2.927718832891247e-05, 'epoch': 1.66}\n",
      "{'loss': 0.3597, 'learning_rate': 2.7204907161803717e-05, 'epoch': 1.82}\n",
      "{'loss': 0.3533, 'learning_rate': 2.5132625994694963e-05, 'epoch': 1.99}\n",
      "{'loss': 0.2808, 'learning_rate': 2.306034482758621e-05, 'epoch': 2.16}\n",
      "{'loss': 0.275, 'learning_rate': 2.0988063660477456e-05, 'epoch': 2.32}\n",
      "{'loss': 0.2685, 'learning_rate': 1.89157824933687e-05, 'epoch': 2.49}\n",
      "{'loss': 0.3095, 'learning_rate': 1.6843501326259946e-05, 'epoch': 2.65}\n",
      "{'loss': 0.303, 'learning_rate': 1.4771220159151194e-05, 'epoch': 2.82}\n",
      "{'loss': 0.2871, 'learning_rate': 1.2698938992042442e-05, 'epoch': 2.98}\n",
      "{'loss': 0.261, 'learning_rate': 1.0626657824933688e-05, 'epoch': 3.15}\n",
      "{'loss': 0.2589, 'learning_rate': 8.554376657824933e-06, 'epoch': 3.32}\n",
      "{'loss': 0.2351, 'learning_rate': 6.4820954907161804e-06, 'epoch': 3.48}\n",
      "{'loss': 0.2396, 'learning_rate': 4.409814323607427e-06, 'epoch': 3.65}\n",
      "{'loss': 0.2601, 'learning_rate': 2.3375331564986738e-06, 'epoch': 3.81}\n",
      "{'loss': 0.2606, 'learning_rate': 2.6525198938992043e-07, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1705.5023, 'train_samples_per_second': 7.074, 'train_steps_per_second': 7.074, 'train_loss': 0.32182312343734015, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import TTTrainArgs\n",
    "args = TTTrainArgs(batch_size=1, num_train_epochs=4)\n",
    "happy_tt.train(\"data/jfleg_train.csv\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6e152e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/12/2022 02:30:59 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7b53868a794850a55f319efb52f7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1a9b12f3aa4b95b114edc65e5d5b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2988\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd87d0c9649472a9e634c43862c44a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loss: 0.5540881752967834\n"
     ]
    }
   ],
   "source": [
    "after_result = happy_tt.eval(\"data/jfleg_eval.csv\")\n",
    "print(\"After loss:\", after_result.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452b70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28353106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\"\n",
    "# correction_model_tag = \"vennify/t5-base-grammar-correction\"\n",
    "# correction_tokenizer = AutoTokenizer.from_pretrained(correction_model_tag)\n",
    "# correction_model = AutoModelForSeq2SeqLM.from_pretrained(correction_model_tag).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8801050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(input_sentence, max_candidates=1):\n",
    "\n",
    "    correction_prefix = \"gec: \"\n",
    "    input_sentence = correction_prefix + input_sentence\n",
    "    input_ids = happy_tt.tokenizer.encode(input_sentence, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    preds = happy_tt.model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=128, \n",
    "        top_k=50, \n",
    "        top_p=1.5, \n",
    "        early_stopping=True,\n",
    "        num_return_sequences=max_candidates)\n",
    "\n",
    "    corrected = set()\n",
    "    for pred in preds:  \n",
    "        corrected.add((happy_tt.tokenizer.decode(pred, skip_special_tokens=True).strip()))\n",
    "    return corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95a93424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_edits(orig, cor):\n",
    "    orig = ANNOTATOR.parse(orig)\n",
    "    cor = ANNOTATOR.parse(cor)\n",
    "    alignment = ANNOTATOR.align(orig, cor)\n",
    "    edits = ANNOTATOR.merge(alignment)\n",
    "\n",
    "    if len(edits) == 0:  \n",
    "        return []\n",
    "\n",
    "    edit_annotations = []\n",
    "    for e in edits:\n",
    "        e = ANNOTATOR.classify(e)\n",
    "        edit_annotations.append((e.type[2:], e.o_str, e.o_start, e.o_end,  e.c_str, e.c_start, e.c_end))\n",
    "            \n",
    "    if len(edit_annotations) > 0:\n",
    "        return edit_annotations\n",
    "    else:    \n",
    "        return []\n",
    "\n",
    "def get_edits(orig, cor):\n",
    "    return _get_edits(orig, cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feaa78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]  He are moving here.\n",
      "[OUTPUT] He are moving here.\n",
      "[Edits]  []\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Input]  the collection of letters was original used by the ancient Romans\n",
      "[OUTPUT] The collection of letters was original and used by the ancient Romans.\n",
      "[Edits]  [('ORTH', 'the', 0, 1, 'The', 0, 1), ('CONJ', '', 6, 6, 'and', 6, 7), ('NOUN', 'Romans', 10, 11, 'Romans.', 11, 12)]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Input]  We enjoys horror movies\n",
      "[OUTPUT] We enjoy horror movies.\n",
      "[Edits]  [('VERB:SVA', 'enjoys', 1, 2, 'enjoy', 1, 2), ('NOUN', 'movies', 3, 4, 'movies.', 3, 4)]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Input]  Anna and Mike is going skiing without us\n",
      "[OUTPUT] Anna and Mike are going skiing without us.\n",
      "[Edits]  [('VERB:SVA', 'is', 3, 4, 'are', 3, 4), ('OTHER', 'us', 7, 8, 'us.', 7, 8)]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Input]  I will eat fish for dinner and drank milk before sleep\n",
      "[OUTPUT] I will eat fish for dinner and drink milk before sleeping.\n",
      "[Edits]  [('VERB:TENSE', 'drank', 7, 8, 'drink', 7, 8), ('NOUN', 'sleep', 10, 11, 'sleeping.', 10, 11)]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Input]  what be the reason for everyone leave the comapny\n",
      "[OUTPUT] What is the reason for everybody to leave the colony?\n",
      "[Edits]  [('ORTH', 'what', 0, 1, 'What', 0, 1), ('VERB:SVA', 'be', 1, 2, 'is', 1, 2), ('NOUN', 'everyone', 5, 6, 'everybody', 5, 6), ('VERB:FORM', '', 6, 6, 'to', 6, 7), ('NOUN', 'comapny', 8, 9, 'colony?', 9, 10)]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "influent_sentences = [\n",
    "    \"He are moving here.\",\n",
    "    \"the collection of letters was original used by the ancient Romans\",\n",
    "    \"We enjoys horror movies\",\n",
    "    \"Anna and Mike is going skiing without us\",\n",
    "    \"I will eat fish for dinner and drank milk before sleep\",\n",
    "    \"what be the reason for everyone leave the comapny\"\n",
    "]\n",
    "\n",
    "\n",
    "corrections = {}\n",
    "\n",
    "for influent_sentence in influent_sentences:\n",
    "    corrected_sentences = correct(influent_sentence, max_candidates=1)\n",
    "    print(\"[Input] \", influent_sentence)\n",
    "    for corrected_sentence in corrected_sentences:\n",
    "      print(\"[OUTPUT]\", corrected_sentence)\n",
    "      print(\"[Edits] \", get_edits(influent_sentence, corrected_sentence))\n",
    "      corrections[influent_sentence] = (corrected_sentence,get_edits(influent_sentence, corrected_sentence))\n",
    "    print(\"-\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f23a5b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to improve your original sentence:\n",
      "\n",
      " He are moving here.\n",
      "\n",
      "try the proposed changes:\n",
      "\n",
      "After changes, the new sentence could read: He are moving here.\n",
      "\n",
      "\n",
      "In order to improve your original sentence:\n",
      "\n",
      " the collection of letters was original used by the ancient Romans\n",
      "\n",
      "try the proposed changes:\n",
      "\n",
      "1: Change 'the' to 'The' (ORTH)\n",
      "\n",
      "\n",
      "2: Change '' to 'and' (CONJ)\n",
      "\n",
      "\n",
      "3: Change 'Romans' to 'Romans.' (NOUN)\n",
      "\n",
      "\n",
      "After changes, the new sentence could read: The collection of letters was original and used by the ancient Romans.\n",
      "\n",
      "\n",
      "In order to improve your original sentence:\n",
      "\n",
      " We enjoys horror movies\n",
      "\n",
      "try the proposed changes:\n",
      "\n",
      "1: Change 'enjoys' to 'enjoy' (VERB:SVA)\n",
      "\n",
      "\n",
      "2: Change 'movies' to 'movies.' (NOUN)\n",
      "\n",
      "\n",
      "After changes, the new sentence could read: We enjoy horror movies.\n",
      "\n",
      "\n",
      "In order to improve your original sentence:\n",
      "\n",
      " Anna and Mike is going skiing without us\n",
      "\n",
      "try the proposed changes:\n",
      "\n",
      "1: Change 'is' to 'are' (VERB:SVA)\n",
      "\n",
      "\n",
      "2: Change 'us' to 'us.' (OTHER)\n",
      "\n",
      "\n",
      "After changes, the new sentence could read: Anna and Mike are going skiing without us.\n",
      "\n",
      "\n",
      "In order to improve your original sentence:\n",
      "\n",
      " I will eat fish for dinner and drank milk before sleep\n",
      "\n",
      "try the proposed changes:\n",
      "\n",
      "1: Change 'drank' to 'drink' (VERB:TENSE)\n",
      "\n",
      "\n",
      "2: Change 'sleep' to 'sleeping.' (NOUN)\n",
      "\n",
      "\n",
      "After changes, the new sentence could read: I will eat fish for dinner and drink milk before sleeping.\n",
      "\n",
      "\n",
      "In order to improve your original sentence:\n",
      "\n",
      " what be the reason for everyone leave the comapny\n",
      "\n",
      "try the proposed changes:\n",
      "\n",
      "1: Change 'what' to 'What' (ORTH)\n",
      "\n",
      "\n",
      "2: Change 'be' to 'is' (VERB:SVA)\n",
      "\n",
      "\n",
      "3: Change 'everyone' to 'everybody' (NOUN)\n",
      "\n",
      "\n",
      "4: Change '' to 'to' (VERB:FORM)\n",
      "\n",
      "\n",
      "5: Change 'comapny' to 'colony?' (NOUN)\n",
      "\n",
      "\n",
      "After changes, the new sentence could read: What is the reason for everybody to leave the colony?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for original_sentence, proposed_edits in corrections.items():\n",
    "    new_sentence, edits = proposed_edits\n",
    "    print(f\"In order to improve your original sentence:\\n\\n {original_sentence}\\n\\ntry the proposed changes:\\n\")\n",
    "    for ind,edit in enumerate(edits):\n",
    "        print(f\"{ind+1}: Change '{edit[1]}' to '{edit[4]}' ({edit[0]})\\n\\n\")\n",
    "    print(f\"After changes, the new sentence could read: {new_sentence}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61266d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model_dir/config.json\n",
      "Model weights saved in model_dir/pytorch_model.bin\n",
      "tokenizer config file saved in model_dir/tokenizer_config.json\n",
      "Special tokens file saved in model_dir/special_tokens_map.json\n",
      "Copy vocab file to model_dir/spiece.model\n"
     ]
    }
   ],
   "source": [
    "happy_tt.save(\"model_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b08ae3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"He are moving here.\": [\"He are moving here.\", []], \"the collection of letters was original used by the ancient Romans\": [\"The collection of letters was original and used by the ancient Romans.\", [[\"ORTH\", \"the\", 0, 1, \"The\", 0, 1], [\"CONJ\", \"\", 6, 6, \"and\", 6, 7], [\"NOUN\", \"Romans\", 10, 11, \"Romans.\", 11, 12]]], \"We enjoys horror movies\": [\"We enjoy horror movies.\", [[\"VERB:SVA\", \"enjoys\", 1, 2, \"enjoy\", 1, 2], [\"NOUN\", \"movies\", 3, 4, \"movies.\", 3, 4]]], \"Anna and Mike is going skiing without us\": [\"Anna and Mike are going skiing without us.\", [[\"VERB:SVA\", \"is\", 3, 4, \"are\", 3, 4], [\"OTHER\", \"us\", 7, 8, \"us.\", 7, 8]]], \"I will eat fish for dinner and drank milk before sleep\": [\"I will eat fish for dinner and drink milk before sleeping.\", [[\"VERB:TENSE\", \"drank\", 7, 8, \"drink\", 7, 8], [\"NOUN\", \"sleep\", 10, 11, \"sleeping.\", 10, 11]]], \"what be the reason for everyone leave the comapny\": [\"What is the reason for everybody to leave the colony?\", [[\"ORTH\", \"what\", 0, 1, \"What\", 0, 1], [\"VERB:SVA\", \"be\", 1, 2, \"is\", 1, 2], [\"NOUN\", \"everyone\", 5, 6, \"everybody\", 5, 6], [\"VERB:FORM\", \"\", 6, 6, \"to\", 6, 7], [\"NOUN\", \"comapny\", 8, 9, \"colony?\", 9, 10]]]}'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(corrections)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d44f63f50595c1a7848f1825bbe5436c15701db73a70b266767f64a3aa574a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
